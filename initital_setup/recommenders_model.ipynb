{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\torch\\lib\\torch_python.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Hamropatro_project\\News_For_You\\News_Recomendation_System\\initital_setup\\recommenders_model.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Hamropatro_project/News_For_You/News_Recomendation_System/initital_setup/recommenders_model.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Hamropatro_project/News_For_You/News_Recomendation_System/initital_setup/recommenders_model.ipynb#ch0000001?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_kernel\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Hamropatro_project/News_For_You/News_Recomendation_System/initital_setup/recommenders_model.ipynb#ch0000001?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Hamropatro_project/News_For_You/News_Recomendation_System/initital_setup/recommenders_model.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Hamropatro_project/News_For_You/News_Recomendation_System/initital_setup/recommenders_model.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39municodedata\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\transformers\\__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     _LazyModule,\n\u001b[0;32m     33\u001b[0m     is_flax_available,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     is_vision_available,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\transformers\\dependency_versions_check.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m pkg \u001b[39min\u001b[39;00m deps:\n\u001b[0;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m pkg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtokenizers\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     35\u001b[0m         \u001b[39m# must be loaded here, or else tqdm check may fail\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_tokenizers_available\n\u001b[0;32m     38\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m     39\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# not required, check version only if installed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\transformers\\file_utils.py:51\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfilelock\u001b[39;00m \u001b[39mimport\u001b[39;00m FileLock\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m HfFolder, Repository, create_repo, list_repo_files, whoami\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPError\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\huggingface_hub\\__init__.py:59\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfile_download\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_download, hf_hub_download, hf_hub_url\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhf_api\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     DatasetSearchArguments,\n\u001b[0;32m     36\u001b[0m     HfApi,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     whoami,\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhub_mixin\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelHubMixin, PyTorchModelHubMixin\n\u001b[0;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minference_api\u001b[39;00m \u001b[39mimport\u001b[39;00m InferenceApi\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mkeras_mixin\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     KerasModelHubMixin,\n\u001b[0;32m     63\u001b[0m     from_pretrained_keras,\n\u001b[0;32m     64\u001b[0m     push_to_hub_keras,\n\u001b[0;32m     65\u001b[0m     save_pretrained_keras,\n\u001b[0;32m     66\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\huggingface_hub\\hub_mixin.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mModelHubMixin\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\torch\\__init__.py:135\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 err \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mWinError(ctypes\u001b[39m.\u001b[39mget_last_error())\n\u001b[0;32m    134\u001b[0m                 err\u001b[39m.\u001b[39mstrerror \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Error loading \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdll\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m or one of its dependencies.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 135\u001b[0m                 \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    137\u001b[0m     kernel32\u001b[39m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    140\u001b[0m \u001b[39m# See Note [Global dependencies]\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Hacker\\Documents\\Sample_Project\\env\\lib\\site-packages\\torch\\lib\\torch_python.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRecommender:\n",
    "\n",
    "    def __init__(self, id_col, tokenization_method = \"nltk\"):\n",
    "\n",
    "        # id_col: Name of columns containing ids(news_id)\n",
    "        # tokenization method contains methods to tokenize the keywords\n",
    "\n",
    "        self.id_col = id_col\n",
    "        if tokenization_method.lower() not in [\"none\"|\"nltk\"|\"bert\"|\"scibert\"]:\n",
    "            raise ValueError(\n",
    "                'Tokenization method must be one of [\"none\" | \"nltk\" | \"bert\" | \"scibert\"]'\n",
    "            )\n",
    "        self.tokenization_method = tokenization_method.lower()\n",
    "\n",
    "\n",
    "        # Initialize other variable used in this class\n",
    "        self.tf = TfidfVectorizer()\n",
    "        self.tfidf_matrix = dict()\n",
    "        self.tokens = dict()\n",
    "        self.stop_words = frozenset()\n",
    "        self.recommendations = dict()\n",
    "        self.top_k_recommendations = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def __clean_text(self, text, for_BERT = False, verbose = False):\n",
    "\n",
    "        try:\n",
    "            text_norm = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "            clean = re.sub(\"<.*?>\", \"\", text_norm)\n",
    "\n",
    "            clean = clean.replace(\"\\n\", \" \")\n",
    "            clean = clean.replace(\"\\t\", \" \")\n",
    "            clean = clean.replace(\"\\r\", \" \")\n",
    "            clean = clean.replace(\"Â\\xa0\", \" \")\n",
    "\n",
    "\n",
    "            clean = re.sub(\n",
    "                r\"([^\\s\\w]|_)+\", \"\", clean\n",
    "            )\n",
    "\n",
    "\n",
    "            if for_BERT is False:\n",
    "                clean = clean.lower()\n",
    "        except Exception:\n",
    "            if verbose is True:\n",
    "                print(\"Cannot clean non-existant text\")\n",
    "            clean = \"\"\n",
    "\n",
    "        return clean\n",
    "    \n",
    "    def clean_dataframe(self, df, cols_to_clean, new_col_name = \"cleaned_text\"):\n",
    "        # df = Dataframe containing the text to clean\n",
    "        # cols_to_clean(list of str) = List of columns to clean by name()\n",
    "        # # new_col_name(str) = Name of the new columns that will contain the cleand text \n",
    "        df = df.replace(np.nan, \"\", regex = True)\n",
    "        df[new_col_name] = df[cols_to_clean].apply(lambda cols: \"\".join(cols), axis = 1)\n",
    "\n",
    "\n",
    "        if self.tokenization_method in ['bert','scibert']:\n",
    "            for_BERT = True\n",
    "        else:\n",
    "            for_BERT = False\n",
    "\n",
    "        df[new_col_name] = df[new_col_name].map(\n",
    "            lambda x: self.__clean_text(x,for_BERT)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def tokenize_text(\n",
    "        self, df_clean, text_col = \"cleaned_text\", ngram_range = (1,3), min_df = 0\n",
    "    ):\n",
    "\n",
    "        vectors = df_clean[text_col]\n",
    "\n",
    "        if self.tokenization_method in ['bert', 'scibert']:\n",
    "            tf = TfidfVectorizer(\n",
    "                analyzer='words',\n",
    "                ngram_range = ngram_range,\n",
    "                min_df = min_df,\n",
    "                stop_words = 'english'\n",
    "            )\n",
    "\n",
    "            if self.tokenization_method == 'bert':\n",
    "                bert_method = \"bert-base-cased\"\n",
    "            elif self.tokenization_method == 'scibert':\n",
    "                bert_method = 'allenai/scibert_scivocab_cased'\n",
    "            \n",
    "            tokenizer = BertTokenizer.from_pretrained(bert_method)\n",
    "\n",
    "            vectors_tokenized = vectors.copy()\n",
    "            for i in range(0, len(vectors)):\n",
    "                vectors_tokenized[i] = \"\".join(tokenizer.tokenize(vectors[i]))\n",
    "        elif self.tokenization_method == 'nltk':\n",
    "            token_dict = {} \n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "            def stem_tokens(tokens, stemmer):\n",
    "                stemmed = []\n",
    "                for item in tokens:\n",
    "                    stemmed.append(stemmer.stem(item))\n",
    "                return stemmed\n",
    "            def tokenize(text):\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                stems = stem_tokens(tokens, stemmer)\n",
    "                return stems\n",
    "\n",
    "            tf = TfidfVectorizer(\n",
    "                tokenizer= tokenize,\n",
    "                analyzer= \"word\",\n",
    "                ngram_range= ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "            vectors_tokenized = vectors\n",
    "\n",
    "        self.tf = tf\n",
    "\n",
    "        return tf, vectors_tokenized\n",
    "\n",
    "    def fit(self, tf, vectors_tokenized):\n",
    "        # tf(TFIDFVectorizer): sklearn.feature_extraction.text.TFIDFVectorizer Object is defined in .tokenize_text()\n",
    "        # vectors_tokenized(pandas.Series): - Each row contains tokens for respective documents seperated by spaces \n",
    "        self.tfidf_matrix = tf.fit_transform(vectors_tokenized)\n",
    "\n",
    "    def get_tokens(self):\n",
    "        try:\n",
    "            self.tokens = self.tf.vocabualary_\n",
    "        except Exception:\n",
    "            self.tokens = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.tokens\n",
    "\n",
    "    def get_stop_words(self):\n",
    "        try:\n",
    "            self.stop_words = self.get_stop_words()\n",
    "        except Exception:\n",
    "            self.tokens = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.tokens\n",
    "\n",
    "\n",
    "    def __create_full_recommnedation_dictionary(self,df_clean):\n",
    "\n",
    "        cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "        sorted_idx = np.argsort(cosine_sim, axix = 1)\n",
    "\n",
    "        data = list(df_clean[self.id_col].values)\n",
    "        len_df_clean = len(df_count)\n",
    "\n",
    "        results  = {}\n",
    "\n",
    "        for idx, row in zip(range(0, len_df_clean), data):\n",
    "            similar_indices = sorted_idx[idx][: -(len_df_clean + 1): -1]\n",
    "            similar_items = [(cosine_sim[idx][i], data[i]) for i in similar_indices]\n",
    "            results[row] = similar_items[1:]\n",
    "\n",
    "        self.recommedations = results\n",
    "\n",
    "    def __organize_results_as_tabular(self, df_clean, k):\n",
    "        items_id = list()\n",
    "        rec_rank = list()\n",
    "        rec_score = list()\n",
    "        rec_item_id = list()\n",
    "\n",
    "    for __item__id in self.recommendations:\n",
    "        rec_based_on = tmp_item_id = __item__id\n",
    "\n",
    "        rec_array = self.recommendations.get(rec_based_on)\n",
    "        temp_rec_score = list(map(lambda x:x[0],rec_array))\n",
    "        temp_rec_id = list(map(lambda x: x[1],rec_array))\n",
    "\n",
    "        item_id.extend([tmp_item_id] * k)\n",
    "        rec_rank.extend(list(range(1, k + 1)))\n",
    "        rec_score.extend(tmp_rec_score[:k])\n",
    "        rec_item_id.extend(tmp_rec_id[:k])\n",
    "\n",
    "    output_dict = {\n",
    "        self.id_col: item_id,\n",
    "        \"rec_rank\": rec_rank,\n",
    "        \"rec_score\": rec_score,\n",
    "        \"rec_\" + self.id_col: rec_item_id,\n",
    "    }\n",
    "\n",
    "    self.top_k_recommendations = pd.DataFrame(output_dict)\n",
    "\n",
    "def recommend_top_k_items(self, df_clean, k=5):\n",
    "    if k > len(df_clean) -1:\n",
    "        raise ValueError(\n",
    "            \"Cannot get more recommendations than there are items. Set k lower.\"\n",
    "        )\n",
    "    self.__create_full_recommnedation_dictionary(df_clean)\n",
    "    self.__organize_results_as_tabular(df_clean, k)\n",
    "\n",
    "    return self.top_k_recommendations\n",
    "\n",
    "def __get_single_item_info(self, metadata, rec_id):\n",
    "    rec_info = metadata.iloc[int(np.where(metadata[self.id_col] == rec_id)[0])]\n",
    "    return rec_info\n",
    "\n",
    "def __make_clickable(self, address):\n",
    "    return '<a href=\"{0}\">{0}</a>'.format(address)\n",
    "\n",
    "def get_top_recommendations(\n",
    "    self, metadata, query_id, cols_to_keep = [], verbose = True\n",
    "):\n",
    "    df = self.top_k_recommendations.loc[self.top_k_recommendations[self.id_col] == query_id].reset_index()\n",
    "    df.drop([self.id_col], axis = 1, inplace = True)\n",
    "\n",
    "    metadata_cols = metadata.columns.values\n",
    "    df[metadata_cols] = df.apply(\n",
    "        lambda row:self.__get_single_item_info(\n",
    "            metadata, row[\"rec_\" + self.id_col]\n",
    "        ),\n",
    "        axis = 1,\n",
    "    )\n",
    "\n",
    "    df.drop([self.id_col], axis = 1, inplace = True)\n",
    "    df = df.ramane(columns = {\"rec_rank\": \"rank\",\n",
    "                                \"rec_score\": \"similarity_score\"})\n",
    "    if len(cols_to_keep) > 0:\n",
    "        cols_to_keep.insert(0, \"similarity_score\")\n",
    "        cols_to_keep.insert(0, \"rank\")\n",
    "        df = df[cols_to_keep]\n",
    "    \n",
    "    if \"url\" in list(map(lambda x:x.lower(), metadata_cols)):\n",
    "        format_ = {\"url\": self.__make_clickable}\n",
    "        df = df.head().style.format(format_)\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        df\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfRecommender:\n",
    "   \n",
    "    def __init__(self, id_col, tokenization_method=\"scibert\"):\n",
    "   \n",
    "        self.id_col = id_col\n",
    "        if tokenization_method.lower() not in [\"none\", \"nltk\", \"bert\", \"scibert\"]:\n",
    "            raise ValueError(\n",
    "                'Tokenization method must be one of [\"none\" | \"nltk\" | \"bert\" | \"scibert\"]'\n",
    "            )\n",
    "        self.tokenization_method = tokenization_method.lower()\n",
    "\n",
    "        # Initialize other variables used in this class\n",
    "        self.tf = TfidfVectorizer()\n",
    "        self.tfidf_matrix = dict()\n",
    "        self.tokens = dict()\n",
    "        self.stop_words = frozenset()\n",
    "        self.recommendations = dict()\n",
    "        self.top_k_recommendations = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def __clean_text(self, text, for_BERT=False, verbose=False):\n",
    "       \n",
    "        try:\n",
    "            # Normalize unicode\n",
    "            text_norm = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "            # Remove HTML tags\n",
    "            # clean = re.sub(\"<.*?>\", \"\", text_norm)\n",
    "\n",
    "            # Remove new line and tabs\n",
    "            clean = clean.replace(\"\\n\", \" \")\n",
    "            clean = clean.replace(\"\\t\", \" \")\n",
    "            clean = clean.replace(\"\\r\", \" \")\n",
    "            clean = clean.replace(\"Â\\xa0\", \"\")  # non-breaking space\n",
    "\n",
    "            # Remove all punctuation and special characters\n",
    "            clean = re.sub(\n",
    "                r\"([^\\s\\w]|_)+\", \"\", clean\n",
    "            )  # noqa W695 invalid escape sequence '\\s'\n",
    "\n",
    "            # If you want to keep some punctuation, see below commented out example\n",
    "            # clean = re.sub(r'([^\\s\\w\\-\\_\\(\\)]|_)+','', clean)\n",
    "\n",
    "            # Skip further processing if the text will be used in BERT tokenization\n",
    "            if for_BERT is False:\n",
    "                # Lower case\n",
    "                clean = clean.lower()\n",
    "        except Exception:\n",
    "            if verbose is True:\n",
    "                print(\"Cannot clean non-existent text\")\n",
    "            clean = \"\"\n",
    "\n",
    "        return clean\n",
    "\n",
    "    def clean_dataframe(self, df, cols_to_clean, new_col_name=\"cleaned_text\"):\n",
    "      \n",
    "        # Collapse the table such that all descriptive text is just in a single column\n",
    "        df = df.replace(np.nan, \"\", regex=True)\n",
    "        df[new_col_name] = df[cols_to_clean].apply(lambda cols: \" \".join(cols), axis=1)\n",
    "\n",
    "        # Check if for BERT tokenization\n",
    "        if self.tokenization_method in [\"bert\", \"scibert\"]:\n",
    "            for_BERT = True\n",
    "        else:\n",
    "            for_BERT = False\n",
    "\n",
    "        # Clean the text in the dataframe\n",
    "        df[new_col_name] = df[new_col_name].map(\n",
    "            lambda x: self.__clean_text(x, for_BERT)\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def tokenize_text(\n",
    "        self, df_clean, text_col=\"cleaned_text\", ngram_range=(1, 3), min_df=0\n",
    "    ):\n",
    "       \n",
    "        vectors = df_clean[text_col]\n",
    "\n",
    "        # If a HuggingFace BERT word tokenization method\n",
    "        if self.tokenization_method in [\"bert\", \"scibert\"]:\n",
    "            # Set vectorizer\n",
    "            tf = TfidfVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "\n",
    "            # Get appropriate transformer name\n",
    "            if self.tokenization_method == \"bert\":\n",
    "                bert_method = \"bert-base-cased\"\n",
    "            elif self.tokenization_method == \"scibert\":\n",
    "                bert_method = \"allenai/scibert_scivocab_cased\"\n",
    "\n",
    "            # Load pre-trained model tokenizer (vocabulary)\n",
    "            tokenizer = BertTokenizer.from_pretrained(bert_method)\n",
    "\n",
    "            # Loop through each item\n",
    "            vectors_tokenized = vectors.copy()\n",
    "            for i in range(0, len(vectors)):\n",
    "                vectors_tokenized[i] = \" \".join(tokenizer.tokenize(vectors[i]))\n",
    "\n",
    "        elif self.tokenization_method == \"nltk\":\n",
    "            # NLTK Stemming\n",
    "            token_dict = {}  # noqa: F841\n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "            def stem_tokens(tokens, stemmer):\n",
    "                stemmed = []\n",
    "                for item in tokens:\n",
    "                    stemmed.append(stemmer.stem(item))\n",
    "                return stemmed\n",
    "\n",
    "            def tokenize(text):\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                stems = stem_tokens(tokens, stemmer)\n",
    "                return stems\n",
    "\n",
    "         \n",
    "            tf = TfidfVectorizer(\n",
    "                tokenizer=tokenize,\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "            vectors_tokenized = vectors\n",
    "\n",
    "        elif self.tokenization_method == \"none\":\n",
    "            # No tokenization applied\n",
    "            tf = TfidfVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "            vectors_tokenized = vectors\n",
    "\n",
    "        # Save to class variable\n",
    "        self.tf = tf\n",
    "\n",
    "        return tf, vectors_tokenized\n",
    "\n",
    "\n",
    "    def fit(self, tf, vectors_tokenized):\n",
    "      \n",
    "        self.tfidf_matrix = tf.fit_transform(vectors_tokenized)\n",
    "\n",
    "\n",
    "    def get_tokens(self):\n",
    "       \n",
    "        try:\n",
    "            self.tokens = self.tf.vocabulary_\n",
    "        except Exception:\n",
    "            self.tokens = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.tokens\n",
    "\n",
    "\n",
    "    def get_stop_words(self):\n",
    "       \n",
    "        try:\n",
    "            self.stop_words = self.tf.get_stop_words()\n",
    "        except Exception:\n",
    "            self.stop_words = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.stop_words\n",
    "\n",
    "\n",
    "    def __create_full_recommendation_dictionary(self, df_clean):\n",
    "\n",
    "        # Similarity measure\n",
    "        cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "        # sorted_idx has the indices that would sort the array.\n",
    "        sorted_idx = np.argsort(cosine_sim, axis=1)\n",
    "\n",
    "        data = list(df_clean[self.id_col].values)\n",
    "        len_df_clean = len(df_clean)\n",
    "\n",
    "        results = {}\n",
    "        for idx, row in zip(range(0, len_df_clean), data):\n",
    "            similar_indices = sorted_idx[idx][: -(len_df_clean + 1) : -1]\n",
    "            similar_items = [(cosine_sim[idx][i], data[i]) for i in similar_indices]\n",
    "            results[row] = similar_items[1:]\n",
    "\n",
    "        # Save to class\n",
    "        self.recommendations = results\n",
    "\n",
    "    def __organize_results_as_tabular(self, df_clean, k):\n",
    "        \n",
    "        # Initialize new dataframe to hold recommendation output\n",
    "        item_id = list()\n",
    "        rec_rank = list()\n",
    "        rec_score = list()\n",
    "        rec_item_id = list()\n",
    "\n",
    "        # For each item\n",
    "        for _item_id in self.recommendations:\n",
    "            # Information about the item we are basing recommendations off of\n",
    "            rec_based_on = tmp_item_id = _item_id\n",
    "\n",
    "            # Get all scores and IDs for items recommended for this current item\n",
    "            rec_array = self.recommendations.get(rec_based_on)\n",
    "            tmp_rec_score = list(map(lambda x: x[0], rec_array))\n",
    "            tmp_rec_id = list(map(lambda x: x[1], rec_array))\n",
    "\n",
    "            # Append multiple values at a time to list\n",
    "            item_id.extend([tmp_item_id] * k)\n",
    "            rec_rank.extend(list(range(1, k + 1)))\n",
    "            rec_score.extend(tmp_rec_score[:k])\n",
    "            rec_item_id.extend(tmp_rec_id[:k])\n",
    "\n",
    "        # Save the output\n",
    "        output_dict = {\n",
    "            self.id_col: item_id,\n",
    "            \"rec_rank\": rec_rank,\n",
    "            \"rec_score\": rec_score,\n",
    "            \"rec_\" + self.id_col: rec_item_id,\n",
    "        }\n",
    "\n",
    "        # Convert to dataframe\n",
    "        self.top_k_recommendations = pd.DataFrame(output_dict)\n",
    "\n",
    "    def recommend_top_k_items(self, df_clean, k=5):\n",
    "       \n",
    "        if k > len(df_clean) - 1:\n",
    "            raise ValueError(\n",
    "                \"Cannot get more recommendations than there are items. Set k lower.\"\n",
    "            )\n",
    "        self.__create_full_recommendation_dictionary(df_clean)\n",
    "        self.__organize_results_as_tabular(df_clean, k)\n",
    "\n",
    "        return self.top_k_recommendations\n",
    "\n",
    "\n",
    "    def __get_single_item_info(self, metadata, rec_id):\n",
    "        \n",
    "        # Return row\n",
    "        rec_info = metadata.iloc[int(np.where(metadata[self.id_col] == rec_id)[0])]\n",
    "\n",
    "        return rec_info\n",
    "\n",
    "    def __make_clickable(self, address):\n",
    "        \n",
    "\n",
    "        return '<a href=\"{0}\">{0}</a>'.format(address)\n",
    "\n",
    "    def get_top_k_recommendations(\n",
    "        self, metadata, query_id, cols_to_keep=[], verbose=True\n",
    "    ):\n",
    "       \n",
    "\n",
    "        # Create subset of dataframe with just recommendations for the item of interest\n",
    "        df = self.top_k_recommendations.loc[\n",
    "            self.top_k_recommendations[self.id_col] == query_id\n",
    "        ].reset_index()\n",
    "\n",
    "        # Remove id_col of query item\n",
    "        df.drop([self.id_col], axis=1, inplace=True)\n",
    "\n",
    "        # Add metadata for each recommended item (rec_<id_col>)\n",
    "        metadata_cols = metadata.columns.values\n",
    "        df[metadata_cols] = df.apply(\n",
    "            lambda row: self.__get_single_item_info(\n",
    "                metadata, row[\"rec_\" + self.id_col]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Remove id col added from metadata (already present from self.top_k_recommendations)\n",
    "        df.drop([self.id_col], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns such that rec_ is no longer appended, for simplicity\n",
    "        df = df.rename(columns={\"rec_rank\": \"rank\", \"rec_score\": \"similarity_score\"})\n",
    "\n",
    "        # Only keep columns of interest\n",
    "        if len(cols_to_keep) > 0:\n",
    "            # Insert our recommendation scoring/ranking columns\n",
    "            cols_to_keep.insert(0, \"similarity_score\")\n",
    "            cols_to_keep.insert(0, \"rank\")\n",
    "            df = df[cols_to_keep]\n",
    "\n",
    "        # Make URLs clickable if they exist\n",
    "        if \"url\" in list(map(lambda x: x.lower(), metadata_cols)):\n",
    "            format_ = {\"url\": self.__make_clickable}\n",
    "            df = df.head().style.format(format_)\n",
    "\n",
    "        if verbose:\n",
    "            df\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ebf1fccd733c983c497b0a0342d21958bfeedfd18a36b387a18420d7b38de9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
