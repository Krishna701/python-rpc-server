{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "# from transformers import BertTokenizer\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfRecommender:\n",
    "\n",
    "    def __init__(self, news_id, tokenization_method=\"scibert\"):\n",
    "      \n",
    "        self.news_id = news_id\n",
    "        if tokenization_method.lower() not in [\"none\", \"nltk\", \"bert\", \"scibert\"]:\n",
    "            raise ValueError(\n",
    "                'Tokenization method must be one of [\"none\" | \"nltk\" | \"bert\" | \"scibert\"]'\n",
    "            )\n",
    "        self.tokenization_method = tokenization_method.lower()\n",
    "\n",
    "        # Initialize other variables used in this class\n",
    "        self.tf = TfidfVectorizer()\n",
    "        self.tfidf_matrix = dict()\n",
    "        self.tokens = dict()\n",
    "        self.stop_words = frozenset()\n",
    "        self.recommendations = dict()\n",
    "        self.top_k_recommendations = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def __clean_text(self, text, for_BERT=False, verbose=False):\n",
    "      \n",
    "        try:\n",
    "            # Normalize unicode\n",
    "            text_norm = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "            # Remove HTML tags\n",
    "            clean = re.sub(\"<.*?>\", \"\", text_norm)\n",
    "\n",
    "            # Remove new line and tabs\n",
    "            clean = clean.replace(\"\\n\", \" \")\n",
    "            clean = clean.replace(\"\\t\", \" \")\n",
    "            clean = clean.replace(\"\\r\", \" \")\n",
    "            clean = clean.replace(\"Ã‚\\xa0\", \"\")  # non-breaking space\n",
    "\n",
    "            # Remove all punctuation and special characters\n",
    "            clean = re.sub(\n",
    "                r\"([^\\s\\w]|_)+\", \"\", clean\n",
    "            )  # noqa W695 invalid escape sequence '\\s'\n",
    "\n",
    "            # If you want to keep some punctuation, see below commented out example\n",
    "            clean = re.sub(r'([^\\s\\w\\-\\_\\(\\)]|_)+','', clean)\n",
    "\n",
    "            # Skip further processing if the text will be used in BERT tokenization\n",
    "            if for_BERT is False:\n",
    "                # Lower case\n",
    "                clean = clean.lower()\n",
    "        except Exception:\n",
    "            if verbose is True:\n",
    "                print(\"Cannot clean non-existent text\")\n",
    "            clean = \"\"\n",
    "\n",
    "        return clean\n",
    "\n",
    "    def clean_dataframe(self, df, cols_to_clean, new_col_name=\"cleaned_text\"):\n",
    "        \n",
    "        \n",
    "        cols_to_clean = [['Title'],['Abstract']]\n",
    "        # Collapse the table such that all descriptive text is just in a single column\n",
    "        df = df.replace(np.nan, \"\", regex=True)\n",
    "        df[new_col_name] = df[cols_to_clean].apply(lambda cols: \" \".join(cols), axis=1)\n",
    "\n",
    "        # Check if for BERT tokenization\n",
    "        if self.tokenization_method in [\"bert\", \"scibert\"]:\n",
    "            for_BERT = True\n",
    "        else:\n",
    "            for_BERT = False\n",
    "\n",
    "        # Clean the text in the dataframe\n",
    "        df[new_col_name] = df[new_col_name].map(\n",
    "            lambda x: self.__clean_text(x, for_BERT)\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def tokenize_text(\n",
    "        self, df_clean, text_col=\"cleaned_text\", ngram_range=(1, 3), min_df=0\n",
    "    ):\n",
    "       \n",
    "        vectors = df_clean[text_col]\n",
    "\n",
    "        # If a HuggingFace BERT word tokenization method\n",
    "        if self.tokenization_method in [\"bert\", \"scibert\"]:\n",
    "            # Set vectorizer\n",
    "            tf = TfidfVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "\n",
    "            # Get appropriate transformer name\n",
    "            if self.tokenization_method == \"bert\":\n",
    "                bert_method = \"bert-base-cased\"\n",
    "            elif self.tokenization_method == \"scibert\":\n",
    "                bert_method = \"allenai/scibert_scivocab_cased\"\n",
    "\n",
    "            # Load pre-trained model tokenizer (vocabulary)\n",
    "            tokenizer = BertTokenizer.from_pretrained(bert_method)\n",
    "\n",
    "            # Loop through each item\n",
    "            vectors_tokenized = vectors.copy()\n",
    "            for i in range(0, len(vectors)):\n",
    "                vectors_tokenized[i] = \" \".join(tokenizer.tokenize(vectors[i]))\n",
    "\n",
    "        elif self.tokenization_method == \"nltk\":\n",
    "            # NLTK Stemming\n",
    "            token_dict = {}  # noqa: F841\n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "            def stem_tokens(tokens, stemmer):\n",
    "                stemmed = []\n",
    "                for item in tokens:\n",
    "                    stemmed.append(stemmer.stem(item))\n",
    "                return stemmed\n",
    "\n",
    "            def tokenize(text):\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                stems = stem_tokens(tokens, stemmer)\n",
    "                return stems\n",
    "\n",
    "         \n",
    "            tf = TfidfVectorizer(\n",
    "                tokenizer=tokenize,\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "            vectors_tokenized = vectors\n",
    "\n",
    "        elif self.tokenization_method == \"none\":\n",
    "            # No tokenization applied\n",
    "            tf = TfidfVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                stop_words=\"english\",\n",
    "            )\n",
    "            vectors_tokenized = vectors\n",
    "\n",
    "        # Save to class variable\n",
    "        self.tf = tf\n",
    "\n",
    "        return tf, vectors_tokenized\n",
    "\n",
    "\n",
    "    def fit(self, tf, vectors_tokenized):\n",
    "      \n",
    "        self.tfidf_matrix = tf.fit_transform(vectors_tokenized)\n",
    "\n",
    "\n",
    "    def get_tokens(self):\n",
    "       \n",
    "        try:\n",
    "            self.tokens = self.tf.vocabulary_\n",
    "        except Exception:\n",
    "            self.tokens = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.tokens\n",
    "\n",
    "\n",
    "    def get_stop_words(self):\n",
    "       \n",
    "        try:\n",
    "            self.stop_words = self.tf.get_stop_words()\n",
    "        except Exception:\n",
    "            self.stop_words = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
    "        return self.stop_words\n",
    "\n",
    "\n",
    "    def __create_full_recommendation_dictionary(self, df_clean):\n",
    "\n",
    "        # Similarity measure\n",
    "        cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "        # sorted_idx has the indices that would sort the array.\n",
    "        sorted_idx = np.argsort(cosine_sim, axis=1)\n",
    "\n",
    "        data = list(df_clean[self.id_col].values)\n",
    "        len_df_clean = len(df_clean)\n",
    "\n",
    "        results = {}\n",
    "        for idx, row in zip(range(0, len_df_clean), data):\n",
    "            similar_indices = sorted_idx[idx][: -(len_df_clean + 1) : -1]\n",
    "            similar_items = [(cosine_sim[idx][i], data[i]) for i in similar_indices]\n",
    "            results[row] = similar_items[1:]\n",
    "\n",
    "        # Save to class\n",
    "        self.recommendations = results\n",
    "\n",
    "    def __organize_results_as_tabular(self, df_clean, k):\n",
    "        \n",
    "        # Initialize new dataframe to hold recommendation output\n",
    "        item_id = list()\n",
    "        rec_rank = list()\n",
    "        rec_score = list()\n",
    "        rec_item_id = list()\n",
    "\n",
    "        # For each item\n",
    "        for _item_id in self.recommendations:\n",
    "            # Information about the item we are basing recommendations off of\n",
    "            rec_based_on = tmp_item_id = _item_id\n",
    "\n",
    "            # Get all scores and IDs for items recommended for this current item\n",
    "            rec_array = self.recommendations.get(rec_based_on)\n",
    "            tmp_rec_score = list(map(lambda x: x[0], rec_array))\n",
    "            tmp_rec_id = list(map(lambda x: x[1], rec_array))\n",
    "\n",
    "            # Append multiple values at a time to list\n",
    "            item_id.extend([tmp_item_id] * k)\n",
    "            rec_rank.extend(list(range(1, k + 1)))\n",
    "            rec_score.extend(tmp_rec_score[:k])\n",
    "            rec_item_id.extend(tmp_rec_id[:k])\n",
    "\n",
    "        # Save the output\n",
    "        output_dict = {\n",
    "            self.id_col: item_id,\n",
    "            \"rec_rank\": rec_rank,\n",
    "            \"rec_score\": rec_score,\n",
    "            \"rec_\" + self.id_col: rec_item_id,\n",
    "        }\n",
    "\n",
    "        # Convert to dataframe\n",
    "        self.top_k_recommendations = pd.DataFrame(output_dict)\n",
    "\n",
    "    def recommend_top_k_items(self, df_clean, k=5):\n",
    "       \n",
    "        if k > len(df_clean) - 1:\n",
    "            raise ValueError(\n",
    "                \"Cannot get more recommendations than there are items. Set k lower.\"\n",
    "            )\n",
    "        self.__create_full_recommendation_dictionary(df_clean)\n",
    "        self.__organize_results_as_tabular(df_clean, k)\n",
    "\n",
    "        return self.top_k_recommendations\n",
    "\n",
    "\n",
    "    def __get_single_item_info(self, metadata, rec_id):\n",
    "        \n",
    "        # Return row\n",
    "        rec_info = metadata.iloc[int(np.where(metadata[self.id_col] == rec_id)[0])]\n",
    "\n",
    "        return rec_info\n",
    "\n",
    "    def __make_clickable(self, address):\n",
    "        \n",
    "\n",
    "        return '<a href=\"{0}\">{0}</a>'.format(address)\n",
    "\n",
    "    def get_top_k_recommendations(\n",
    "        self, metadata, query_id, cols_to_keep=[], verbose=True\n",
    "    ):\n",
    "       \n",
    "\n",
    "        # Create subset of dataframe with just recommendations for the item of interest\n",
    "        df = self.top_k_recommendations.loc[\n",
    "            self.top_k_recommendations[self.id_col] == query_id\n",
    "        ].reset_index()\n",
    "\n",
    "        # Remove id_col of query item\n",
    "        df.drop([self.id_col], axis=1, inplace=True)\n",
    "\n",
    "        # Add metadata for each recommended item (rec_<id_col>)\n",
    "        metadata_cols = metadata.columns.values\n",
    "        df[metadata_cols] = df.apply(\n",
    "            lambda row: self.__get_single_item_info(\n",
    "                metadata, row[\"rec_\" + self.id_col]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Remove id col added from metadata (already present from self.top_k_recommendations)\n",
    "        df.drop([self.id_col], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns such that rec_ is no longer appended, for simplicity\n",
    "        df = df.rename(columns={\"rec_rank\": \"rank\", \"rec_score\": \"similarity_score\"})\n",
    "\n",
    "        # Only keep columns of interest\n",
    "        if len(cols_to_keep) > 0:\n",
    "            # Insert our recommendation scoring/ranking columns\n",
    "            cols_to_keep.insert(0, \"similarity_score\")\n",
    "            cols_to_keep.insert(0, \"rank\")\n",
    "            df = df[cols_to_keep]\n",
    "\n",
    "        # Make URLs clickable if they exist\n",
    "        if \"url\" in list(map(lambda x: x.lower(), metadata_cols)):\n",
    "            format_ = {\"url\": self.__make_clickable}\n",
    "            df = df.head().style.format(format_)\n",
    "\n",
    "        if verbose:\n",
    "            df\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2374985365.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install nltk\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "04ebf1fccd733c983c497b0a0342d21958bfeedfd18a36b387a18420d7b38de9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
